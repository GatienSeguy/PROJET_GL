# ====================================
# IMPORTs
#export KMP_DUPLICATE_LIB_OK=TRUEDD
#export OMP_NUM_THREADS=1
#python -m uvicorn SERVEUR_IA.test_main:app --host 0.0.0.0 --port 8000 --reload
# ====================================
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi.encoders import jsonable_encoder
from fastapi import HTTPException

import json
import torch
import numpy as np

# Train Modele
from .trains.training_MLP import train_MLP
from .trains.training_CNN import train_CNN
from .trains.training_LSTM import train_LSTM

# Test
from .test.test_testing import test_model_validation, compute_residual_std_from_validation
from .test.test_prediction_strategie import (
    predict_multistep,
    compare_all_strategies,
    PredictionStrategy, 
    PredictionConfig
)

from typing import List, Optional, Dict, Any, Tuple

from .classes import (
    TimeSeriesData,
    Parametres_archi_reseau_MLP,
    Parametres_archi_reseau_CNN,
    Parametres_archi_reseau_LSTM,
    Tx_choix_dataset,
    PaquetComplet,
    newDatasetRequest,
    deleteDatasetRequest,
    AddDatasetPacket
    )

from .test_fonctions_pour_main import (
    filter_series_by_dates,
    build_supervised_tensors_with_step,
    split_train_test,
    split_train_val_test,  # NOUVEAU
    sse,
    normalize_data,
    create_inverse_function
)

import os
import requests

DATA_SERVER_URL = os.getenv("DATA_SERVER_URL", "http://192.168.1.190:8001")
# DATA_SERVER_URL = os.getenv("DATA_SERVER_URL", "http://138.231.152.52:8001")

app = FastAPI()

last_config_tempo = None
last_config_TimeSeries = None
last_config_series = None

payload_json = {"timestamps": [], "values": []}

# ====================================
# VARIABLES GLOBALES POUR LE MODÃˆLE ENTRAÃŽNÃ‰
# ====================================
trained_model_state = {
    "model": None,           # Le modÃ¨le PyTorch entraÃ®nÃ©
    "norm_params": None,     # ParamÃ¨tres de normalisation
    "inverse_fn": None,      # Fonction inverse pour dÃ©normaliser
    "window_size": None,     # Taille de la fenÃªtre d'entrÃ©e
    "residual_std": None,    # Ã‰cart-type des rÃ©sidus (pour IC)
    "model_type": None,      # Type de modÃ¨le (mlp, lstm, cnn)
    "device": "cpu",         # Device utilisÃ©
    "is_trained": False,     # Flag indiquant si un modÃ¨le est disponible
}


# ====================================
# CLASSE GESTION DATASETS
# ====================================
class DatasetManager:
    """GÃ¨re la communication avec le serveur Data"""
    
    def __init__(self, data_server_url: str = DATA_SERVER_URL):
        self.data_server_url = data_server_url
    
    def get_available_datasets(self) -> List[str]:
        try:
            url = f"{self.data_server_url}/datasets/list"
            response = requests.get(url, timeout=100)
            response.raise_for_status()
            data = response.json()
            return data.get("datasets", [])
        except requests.exceptions.RequestException as e:
            raise Exception(f"Erreur lors de la rÃ©cupÃ©ration des datasets: {str(e)}")
    
    def fetch_dataset(self, dataset_name: str, date_start: str, date_end: str) -> TimeSeriesData:
        try:
            url = f"{self.data_server_url}/datasets/fetch"
            payload = {"name": dataset_name, "dates": [date_start, date_end]}
            response = requests.post(url, json=payload, timeout=10000)
            response.raise_for_status()
            data_json = response.json()
            if "timestamps" not in data_json or "values" not in data_json:
                raise ValueError(f"Format de donnÃ©es invalide reÃ§u du serveur Data")
            time_series = TimeSeriesData(**data_json)
            return time_series
        except requests.exceptions.RequestException as e:
            raise Exception(f"Erreur lors de la rÃ©cupÃ©ration du dataset '{dataset_name}': {str(e)}")
        except ValueError as e:
            raise Exception(f"Format de donnÃ©es invalide: {str(e)}")


# ====================================
# CLASSE PRINCIPALE - PIPELINE 3 PHASES
# ====================================
class TrainingPipeline:
    """Pipeline d'entraÃ®nement avec 3 phases : train / validation / test prÃ©dictif"""

    def __init__(self, payload: PaquetComplet, payload_model: dict, time_series_data: Optional[TimeSeriesData] = None):
        
        self.stop_flag = False

        self.cfg = payload
        self.payload_model = payload_model
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu")

        # Variables d'Ã©tat
        self.series = time_series_data
        self.cfg_model = None
        self.X = None
        self.y = None

        # NOUVEAU : 3 splits
        self.X_train = None
        self.y_train = None
        self.X_val = None
        self.y_val = None
        self.X_test = None
        self.y_test = None
        self.split_info = None
        
        self.norm_params = None
        self.inverse_fn = None
        self.model_trained = None
        self.residual_std = None  # NOUVEAU : pour le halo
        
        # Ratios de split (paramÃ©trable)
        self.train_ratio = 0.8
        self.val_ratio = 0.1
        self.test_ratio = 0.1

    # ====================================
    # 1) CHARGEMENT DES DONNÃ‰ES
    # ====================================
    def load_data(self, payload_json) -> TimeSeriesData:
        data_json = payload_json
        self.series = TimeSeriesData(**data_json)
        return self.series

    # ====================================
    # 2) CONFIGURATION DU MODÃˆLE
    # ====================================
    def setup_model_config(self):
        model_type = self.cfg.Parametres_choix_reseau_neurones.modele.lower()
        
        if model_type == "mlp":
            self.cfg_model = Parametres_archi_reseau_MLP(**self.payload_model)
        elif model_type == "cnn":
            self.cfg_model = Parametres_archi_reseau_CNN(**self.payload_model)
        elif model_type == "lstm":
            self.cfg_model = Parametres_archi_reseau_LSTM(**self.payload_model)
        else:
            raise ValueError(f"ModÃ¨le inconnu: {model_type}")
        
        return self.cfg_model

    # ====================================
    # 3) PRÃ‰TRAITEMENT
    # ====================================
    def preprocess_data(self) -> Tuple[torch.Tensor, torch.Tensor]:
        horizon = 1
        if self.cfg and self.cfg.Parametres_temporels and self.cfg.Parametres_temporels.horizon:
            horizon = max(1, int(self.cfg.Parametres_temporels.horizon))
        
        X, y = build_supervised_tensors_with_step(
            self.series.values,
            horizon=horizon
        )
        
        if X.numel() == 0:
            raise ValueError("(X,y) vide")
        
        self.X = X
        self.y = y
        return X, y

    def normalize_data(self):
        all_data = torch.cat([self.X.flatten(), self.y.flatten()])
        all_data_normalized, self.norm_params = normalize_data(all_data, method="standardization")
        
        total_X = self.X.numel()
        X_normalized = all_data_normalized[:total_X].reshape(self.X.shape)
        y_normalized = all_data_normalized[total_X:].reshape(self.y.shape)
        
        self.inverse_fn = create_inverse_function(self.norm_params)
        
        self.X = X_normalized
        self.y = y_normalized

    # ====================================
    # NOUVEAU : SPLIT 3 PARTIES
    # ====================================
    def split_data_three_way(self):
        """Divise les donnÃ©es en train/validation/test (80/10/10 par dÃ©faut)"""
        
        # RÃ©cupÃ©rer les ratios depuis la config si prÃ©sents
        if self.cfg and self.cfg.Parametres_temporels and self.cfg.Parametres_temporels.portion_decoupage:
            self.train_ratio = float(self.cfg.Parametres_temporels.portion_decoupage)
            remaining = 1.0 - self.train_ratio
            self.val_ratio = remaining / 2
            self.test_ratio = remaining / 2
        
        (self.X_train, self.y_train, 
         self.X_val, self.y_val, 
         self.X_test, self.y_test, 
         self.split_info) = split_train_val_test(
            self.X, self.y,
            train_ratio=self.train_ratio,
            val_ratio=self.val_ratio,
            test_ratio=self.test_ratio
        )
        
        return self.split_info

    # ====================================
    # 4) RESHAPE POUR LE MODÃˆLE
    # ====================================
    def reshape_data_for_model(self, model_type: str):
        if model_type == "lstm":
            if self.X_train.ndim == 2:
                self.X_train = self.X_train.unsqueeze(-1)
            if self.X_val.ndim == 2 and self.X_val.numel() > 0:
                self.X_val = self.X_val.unsqueeze(-1)
            if self.X_test.ndim == 2 and self.X_test.numel() > 0:
                self.X_test = self.X_test.unsqueeze(-1)
        
        elif model_type == "cnn":
            if self.X_train.ndim == 2:
                self.X_train = self.X_train.unsqueeze(1)
            if self.X_val.ndim == 2 and self.X_val.numel() > 0:
                self.X_val = self.X_val.unsqueeze(1)
            if self.X_test.ndim == 2 and self.X_test.numel() > 0:
                self.X_test = self.X_test.unsqueeze(1)

    # ====================================
    # 5-8) CONFIGURATION (inchangÃ©)
    # ====================================
    def setup_mlp_architecture(self) -> Dict[str, Any]:
        params = {
            "hidden_size": 128,
            "nb_couches": 2,
            "dropout_rate": 0.0,
            "activation": "relu",
            "use_batchnorm": False,
        }
        if self.cfg_model.hidden_size is not None:
            params["hidden_size"] = int(self.cfg_model.hidden_size)
        if self.cfg_model.nb_couches is not None:
            params["nb_couches"] = int(self.cfg_model.nb_couches)
        if self.cfg_model.dropout_rate is not None:
            params["dropout_rate"] = float(self.cfg_model.dropout_rate)
        if self.cfg_model.fonction_activation is not None:
            act_map = {"ReLU": "relu", "GELU": "gelu", "tanh": "tanh", "sigmoid": "sigmoid", "leaky_relu": "leaky_relu"}
            params["activation"] = act_map.get(self.cfg_model.fonction_activation, "relu")
        return params

    def setup_cnn_architecture(self) -> Dict[str, Any]:
        params = {
            "hidden_size": 128, "nb_couches": 2, "dropout_rate": 0.0,
            "activation": "relu", "use_batchnorm": False,
            "kernel_size": 3, "stride": 1, "padding": 1,
        }
        if self.cfg_model.hidden_size is not None:
            params["hidden_size"] = int(self.cfg_model.hidden_size)
        if self.cfg_model.nb_couches is not None:
            params["nb_couches"] = int(self.cfg_model.nb_couches)
        if self.cfg_model.fonction_activation is not None:
            act_map = {"ReLU": "relu", "GELU": "gelu", "tanh": "tanh", "sigmoid": "sigmoid", "leaky_relu": "leaky_relu"}
            params["activation"] = act_map.get(self.cfg_model.fonction_activation, "relu")
        if self.cfg_model.kernel_size is not None:
            params["kernel_size"] = int(self.cfg_model.kernel_size)
        if self.cfg_model.stride is not None:
            params["stride"] = int(self.cfg_model.stride)
        if self.cfg_model.padding is not None:
            params["padding"] = int(self.cfg_model.padding)
        return params

    def setup_lstm_architecture(self) -> Dict[str, Any]:
        params = {"hidden_size": 128, "nb_couches": 2, "bidirectional": False, "batch_first": True}
        if self.cfg_model.hidden_size is not None:
            params["hidden_size"] = int(self.cfg_model.hidden_size)
        if self.cfg_model.nb_couches is not None:
            params["nb_couches"] = int(self.cfg_model.nb_couches)
        if self.cfg_model.bidirectional is not None:
            params["bidirectional"] = bool(self.cfg_model.bidirectional)
        if self.cfg_model.batch_first is not None:
            params["batch_first"] = bool(self.cfg_model.batch_first)
        return params

    def setup_architecture(self, model_type: str) -> Dict[str, Any]:
        if model_type == "mlp":
            return self.setup_mlp_architecture()
        elif model_type == "cnn":
            return self.setup_cnn_architecture()
        elif model_type == "lstm":
            return self.setup_lstm_architecture()
        else:
            raise ValueError(f"ModÃ¨le inconnu: {model_type}")

    def setup_loss(self) -> str:
        loss_name = "mse"
        if self.cfg and self.cfg.Parametres_choix_loss_fct and self.cfg.Parametres_choix_loss_fct.fonction_perte:
            loss_name = self.cfg.Parametres_choix_loss_fct.fonction_perte.lower()
        return loss_name

    def setup_optimizer(self) -> Tuple[str, float, float]:
        optimizer_name, learning_rate, weight_decay = "adam", 1e-3, 0.0
        if self.cfg and self.cfg.Parametres_optimisateur:
            if self.cfg.Parametres_optimisateur.optimisateur:
                optimizer_name = self.cfg.Parametres_optimisateur.optimisateur.lower()
            if self.cfg.Parametres_optimisateur.learning_rate is not None:
                learning_rate = float(self.cfg.Parametres_optimisateur.learning_rate)
            if self.cfg.Parametres_optimisateur.decroissance is not None:
                weight_decay = float(self.cfg.Parametres_optimisateur.decroissance)
        return optimizer_name, learning_rate, weight_decay

    def setup_training(self) -> Tuple[int, int]:
        epochs, batch_size = 10, 64
        if self.cfg and self.cfg.Parametres_entrainement:
            if self.cfg.Parametres_entrainement.nb_epochs is not None:
                epochs = int(self.cfg.Parametres_entrainement.nb_epochs)
            if self.cfg.Parametres_entrainement.batch_size is not None:
                batch_size = int(self.cfg.Parametres_entrainement.batch_size)
        return epochs, batch_size

    # ====================================
    # 9) ENTRAÃŽNEMENT
    # ====================================
    def create_training_generator(self, model_type, arch_params, loss_name, 
                                  optimizer_name, learning_rate, weight_decay, epochs, batch_size):
        if model_type == "mlp":
            return train_MLP(
                self.X_train, self.y_train,
                hidden_size=arch_params["hidden_size"],
                nb_couches=arch_params["nb_couches"],
                dropout_rate=arch_params["dropout_rate"],
                activation=arch_params["activation"],
                use_batchnorm=arch_params["use_batchnorm"],
                loss_name=loss_name,
                optimizer_name=optimizer_name,
                learning_rate=learning_rate,
                weight_decay=weight_decay,
                batch_size=batch_size,
                epochs=epochs,
                device=self.device,
            )
        elif model_type == "cnn":
            return train_CNN(
                self.X_train, self.y_train,
                hidden_size=arch_params["hidden_size"],
                nb_couches=arch_params["nb_couches"],
                kernel_size=arch_params["kernel_size"],
                stride=arch_params["stride"],
                padding=arch_params["padding"],
                activation=arch_params["activation"],
                use_batchnorm=arch_params["use_batchnorm"],
                loss_name=loss_name,
                optimizer_name=optimizer_name,
                learning_rate=learning_rate,
                weight_decay=weight_decay,
                batch_size=batch_size,
                epochs=epochs,
                device=self.device,
            )
        elif model_type == "lstm":
            return train_LSTM(
                self.X_train, self.y_train,
                hidden_size=arch_params["hidden_size"],
                nb_couches=arch_params["nb_couches"],
                bidirectional=arch_params["bidirectional"],
                batch_first=arch_params["batch_first"],
                loss_name=loss_name,
                optimizer_name=optimizer_name,
                learning_rate=learning_rate,
                weight_decay=weight_decay,
                batch_size=batch_size,
                epochs=epochs,
                device=self.device,
            )
        else:
            raise ValueError(f"ModÃ¨le inconnu: {model_type}")

    def run_training(self):
        """Phase 1 : EntraÃ®nement"""
        global stop_training_flag

        model_type = self.cfg.Parametres_choix_reseau_neurones.modele.lower()
        arch_params = self.setup_architecture(model_type)
        loss_name = self.setup_loss()
        optimizer_name, learning_rate, weight_decay = self.setup_optimizer()
        epochs, batch_size = self.setup_training()
        
        gen = self.create_training_generator(
            model_type, arch_params, loss_name,
            optimizer_name, learning_rate, weight_decay, epochs, batch_size
        )
        
        try:
            while True:
                if stop_training_flag:
                    stop_training_flag = False
                    yield {"type": "stopped", "message": "EntraÃ®nement arrÃªtÃ©"}
                    break

                msg = next(gen)
                yield msg
        except StopIteration as e:
            self.model_trained = e.value

    # ====================================
    # NOUVEAU : PHASE 2 - VALIDATION
    # ====================================
    def run_validation(self):
        """Phase 2 : Validation (teacher forcing) sur les 10% aprÃ¨s train"""
        if self.model_trained is None:
            yield {"type": "warn", "message": "ModÃ¨le non rÃ©cupÃ©rÃ© (validation sautÃ©e)."}
            return
        
        if self.X_val is None or self.X_val.numel() == 0:
            yield {"type": "warn", "message": "Pas de donnÃ©es de validation."}
            return
        
        idx_val_start = self.split_info["idx_val_start"]
        
        val_y_true = []
        val_y_pred = []
        
        for evt in test_model_validation(
            self.model_trained,
            self.X_val,
            self.y_val,
            device=self.device,
            batch_size=256,
            inverse_fn=self.inverse_fn,
            idx_start=idx_val_start
        ):
            # Collecter les prÃ©dictions pour calculer residual_std
            if evt["type"] == "val_end":
                val_y_true = evt.get("all_true", [])
                val_y_pred = evt.get("all_predictions", [])
                
                # Calculer l'Ã©cart-type des rÃ©sidus
                self.residual_std = compute_residual_std_from_validation(val_y_true, val_y_pred)
                evt["residual_std"] = self.residual_std
            
            yield evt

    # ====================================
    # PHASE 3 - TEST PRÃ‰DICTIF (Toutes stratÃ©gies)
    # ====================================
    def run_prediction_test(self, strategy: str = "one_step"):
        """
        Phase 3 : Test prÃ©dictif avec choix de la stratÃ©gie.
        
        StratÃ©gies disponibles:
        - "one_step": PrÃ©diction 1 pas + recalibration immÃ©diate (RECOMMANDÃ‰)
        - "recalibration": PrÃ©diction rÃ©cursive + recalibration tous les N pas
        - "recursive": PrÃ©diction autorÃ©gressive pure (diverge vite)
        - "direct": Multi-horizon (nÃ©cessite modÃ¨le spÃ©cial)
        
        BasÃ© sur Taieb & Hyndman (2014).
        """
        if self.model_trained is None:
            yield {"type": "warn", "message": "ModÃ¨le non rÃ©cupÃ©rÃ© (test pred sautÃ©)."}
            return
        
        idx_test_start = self.split_info["idx_test_start"]
        n_test = self.split_info["n_test"]
        
        if n_test == 0:
            yield {"type": "warn", "message": "Pas de donnÃ©es pour test prÃ©dictif."}
            return
        
        # Taille de la fenÃªtre
        window_size = self.X.shape[1] if self.X.ndim >= 2 else 1
        
        # Vraies valeurs
        y_true_values = self.series.values[idx_test_start:idx_test_start + n_test]
        
        # Configuration selon la stratÃ©gie
        strategy_map = {
            "one_step": PredictionStrategy.ONE_STEP,
            "recalibration": PredictionStrategy.RECALIBRATION,
            "recursive": PredictionStrategy.RECURSIVE,
            "direct": PredictionStrategy.DIRECT,
        }
        
        pred_strategy = strategy_map.get(strategy, PredictionStrategy.RECALIBRATION)
        
        config = PredictionConfig(
            strategy=pred_strategy,
            recalib_every=min(10, max(1, n_test // 20)) if n_test > 20 else 5,
            max_horizon=min(50, n_test),
            direct_horizon=10,
            confidence_level=0.95
        )
        
        print(f"[PRED] StratÃ©gie: {pred_strategy.value.upper()}")
        
        for evt in predict_multistep(
            model=self.model_trained,
            values=self.series.values,
            norm_stats=self.norm_params,
            window_size=window_size,
            n_steps=n_test,
            device=self.device,
            inverse_fn=self.inverse_fn,
            config=config,
            residual_std=self.residual_std,
            y_true=y_true_values,
            idx_start=idx_test_start,
        ):
            yield evt
    
    # ====================================
    # COMPARAISON DE TOUTES LES STRATÃ‰GIES
    # ====================================
    def run_all_strategies_comparison(self):
        """
        Compare toutes les stratÃ©gies de prÃ©diction.
        
        Retourne les mÃ©triques de chaque stratÃ©gie pour comparaison.
        """
        if self.model_trained is None:
            yield {"type": "warn", "message": "ModÃ¨le non rÃ©cupÃ©rÃ©."}
            return
        
        idx_test_start = self.split_info["idx_test_start"]
        n_test = self.split_info["n_test"]
        window_size = self.X.shape[1] if self.X.ndim >= 2 else 1
        y_true_values = self.series.values[idx_test_start:idx_test_start + n_test]
        
        yield {"type": "comparison_start", "n_strategies": 3}
        
        results = compare_all_strategies(
            model=self.model_trained,
            values=self.series.values,
            norm_stats=self.norm_params,
            window_size=window_size,
            n_steps=n_test,
            device=self.device,
            inverse_fn=self.inverse_fn,
            residual_std=self.residual_std,
            y_true=y_true_values,
            idx_start=idx_test_start,
            recalib_every=10,
            max_horizon=min(50, n_test),
        )
        
        yield {
            "type": "comparison_end",
            "results": results,
            "idx_start": idx_test_start,
            "n_steps": n_test
        }

    # ====================================
    # 11) ORCHESTRATION COMPLÃˆTE
    # ====================================
    def execute_full_pipeline(self):
        """Orchestre le pipeline complet : train â†’ validation â†’ test prÃ©dictif"""
        global stop_training_flag
        stop_training_flag = False
        try:
            # Chargement des donnÃ©es
            self.load_data(payload_json=payload_json)
            
            # Configuration du modÃ¨le
            self.setup_model_config()
            
            # PrÃ©traitement
            self.preprocess_data()
            self.normalize_data()
            
            # NOUVEAU : Split 3 parties
            split_info = self.split_data_three_way()
            
            # Reshape des donnÃ©es
            model_type = self.cfg.Parametres_choix_reseau_neurones.modele.lower()
            self.reshape_data_for_model(model_type)
            
            # Information de split (NOUVEAU format)
            yield sse({
                "type": "split_info",
                "n_train": split_info["n_train"],
                "n_val": split_info["n_val"],
                "n_test": split_info["n_test"],
                "idx_val_start": split_info["idx_val_start"],
                "idx_test_start": split_info["idx_test_start"],
            })
            
            # Envoyer la sÃ©rie complÃ¨te pour l'affichage
            yield sse({
                "type": "serie_complete",
                "values": self.series.values,
            })
            
            # ========== PHASE 1 : ENTRAÃŽNEMENT ==========
            yield sse({"type": "phase", "phase": "train", "status": "start"})
            
            for msg in self.run_training():
                yield sse(msg)
            
            yield sse({"type": "phase", "phase": "train", "status": "end"})
            
            # ========== PHASE 2 : VALIDATION ==========
            yield sse({"type": "phase", "phase": "validation", "status": "start"})
            
            val_predictions = []
            val_true = []
            val_metrics = None
            
            for msg in self.run_validation():
                if msg["type"] == "val_pair":
                    val_predictions.append(msg["yhat"])
                    val_true.append(msg["y"])
                elif msg["type"] == "val_end":
                    val_metrics = msg.get("metrics")
                yield sse(msg)
            
            yield sse({"type": "phase", "phase": "validation", "status": "end"})
            
            # ========== PHASE 3 : TEST PRÃ‰DICTIF ==========
            yield sse({"type": "phase", "phase": "prediction", "status": "start"})
            
            pred_predictions = []
            pred_low = []
            pred_high = []
            pred_true = []
            pred_metrics = None
            
            for msg in self.run_prediction_test():
                if msg["type"] == "pred_point":
                    pred_predictions.append(msg["yhat"])
                    pred_low.append(msg["low"])
                    pred_high.append(msg["high"])
                    if msg["y"] is not None:
                        pred_true.append(msg["y"])
                elif msg["type"] == "pred_end":
                    pred_metrics = msg.get("metrics")
                yield sse(msg)
            
            yield sse({"type": "phase", "phase": "prediction", "status": "end"})
            
            # ========== SAUVEGARDER LE MODÃˆLE ENTRAÃŽNÃ‰ ==========
            global trained_model_state
            window_size = self.X.shape[1] if self.X.ndim >= 2 else 1
            trained_model_state = {
                "model": self.model_trained,
                "norm_params": self.norm_params,
                "inverse_fn": self.inverse_fn,
                "window_size": window_size,
                "residual_std": self.residual_std if self.residual_std else 0.1,
                "model_type": self.cfg.Parametres_choix_reseau_neurones.modele.lower(),
                "device": str(self.device),
                "is_trained": True,
            }
            print(f"âœ… ModÃ¨le sauvegardÃ© en mÃ©moire (window_size={window_size}, type={trained_model_state['model_type']})")
            
            # ========== DONNÃ‰ES FINALES POUR L'AFFICHAGE ==========
            yield sse({
                "type": "final_plot_data",
                "series_complete": self.series.values,
                "val_predictions": val_predictions,
                "val_true": val_true,
                "pred_predictions": pred_predictions,
                "pred_low": pred_low,
                "pred_high": pred_high,
                "pred_true": pred_true,
                "idx_val_start": split_info["idx_val_start"],
                "idx_test_start": split_info["idx_test_start"],
                "val_metrics": val_metrics,
                "pred_metrics": pred_metrics,
            })
            
            yield sse({"type": "fin_pipeline", "done": 1})
        
        except Exception as e:
            yield sse({"type": "error", "message": str(e)})


# ====================================
# ROUTES
# ====================================
@app.post("/datasets/info_all")
def proxy_get_dataset_list(payload: dict):
    print("Message reÃ§u depuis UI :", payload)
    if payload.get("message") != "choix dataset":
        return {"status": "error", "message": "Message invalide"}
    try:
        url = f"{DATA_SERVER_URL}/datasets/info_all"
        response = requests.post(url, json=payload, timeout=1000)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print("Exception cÃ´tÃ© IA :", e)
        return {"status": "error", "message": str(e)}


def extract_timestamps_values(obj):
    if isinstance(obj, dict):
        if "timestamps" in obj and "values" in obj:
            return obj["timestamps"], obj["values"]
        for v in obj.values():
            ts, vals = extract_timestamps_values(v)
            if ts is not None and vals is not None:
                return ts, vals
    elif isinstance(obj, list):
        for item in obj:
            ts, vals = extract_timestamps_values(item)
            if ts is not None and vals is not None:
                return ts, vals
    return None, None


@app.post("/datasets/fetch_dataset")
def proxy_fetch_dataset(payload: dict):
    global payload_json
    print("Message reÃ§u depuis UI pour fetch_dataset :", payload)
    try:
        url = f"{DATA_SERVER_URL}/datasets/data_solo"
        response = requests.post(url, json=payload, timeout=1000)
        response.raise_for_status()
        data = response.json()
        print("RÃ©ponse brute DATA_SERVER :", data)
        ts, vals = extract_timestamps_values(data)
        if ts is None or vals is None:
            return {
                "status": "error",
                "message": "Format de donnÃ©es inattendu depuis DATA_SERVER"
            }
        payload_json = {"timestamps": ts, "values": vals}
        print(f"[IA] Dataset chargÃ© dans payload_json : {len(ts)} points")
        return {"status": "success", "data": payload_json}
    except Exception as e:
        print("Exception cÃ´tÃ© IA lors du fetch_dataset :", e)
        return {"status": "error", "message": str(e)}



@app.post("/datasets/data_add_proxy")
def proxy_add_dataset(payload: dict):
    url = f"{DATA_SERVER_URL}/datasets/data_add"
    data_to_send = jsonable_encoder(payload)
    print("=== ENVOYÃ‰ AU SERVEUR DATA ===")
    # print(data_to_send)

    try:
        response = requests.post(url, json=data_to_send, timeout=1000)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur de connexion au serveur DATA: {e}")

    print("=== RÃ‰PONSE SERVEUR DATA ===")
    # print("Status:", response.status_code)
    # print("Body  :", response.text)

    if response.status_code >= 400:
        raise HTTPException(
            status_code=response.status_code,
            detail=f"Erreur serveur DATA: {response.text}"
        )

    return response.json()


@app.post("/datasets/add_dataset")
def add_dataset_proxy(packet: AddDatasetPacket):
    url = f"{DATA_SERVER_URL}/datasets/add_dataset"
    print("PAQUET D'AJOUTER DS IA", packet)
    print("FORWARD URL =", url)

    try:
        out_json = packet.model_dump(mode="json")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Serialization error in IA: {repr(e)}")

    try:
        print("âž¡ï¸ Forwarding to DATA server...")
        resp = requests.post(url, json=out_json, timeout=60)
        print("â¬…ï¸ DATA server status:", resp.status_code)
        print("â¬…ï¸ DATA server headers:", dict(resp.headers))
        print("â¬…ï¸ DATA server body (first 1000 chars):", resp.text[:1000])
    except requests.RequestException as e:
        raise HTTPException(status_code=502, detail=f"Dataset server unreachable: {repr(e)}")

    if not resp.ok:
        # propage l'erreur DATA
        try:
            detail = resp.json()
        except Exception:
            detail = resp.text
        raise HTTPException(status_code=resp.status_code, detail=detail)

    # âœ… ICI: parse JSON safe (sinon 500 muet)
    try:
        return resp.json()
    except Exception as e:
        raise HTTPException(
            status_code=502,
            detail={
                "error": "DATA server returned non-JSON response",
                "exception": repr(e),
                "status_code": resp.status_code,
                "content_type": resp.headers.get("content-type"),
                "body_preview": resp.text[:1000],
            },
        )



@app.post("/datasets/data_suppression_proxy")
def proxy_suppression_dataset(payload: deleteDatasetRequest):
    print("Message reÃ§u depuis UI pour suppression:", payload.name)
    url = f"{DATA_SERVER_URL}/datasets/data_supression"
    
    # âœ… SÃ©rialiser l'objet Pydantic en dict
    payload_dict = payload.model_dump() if hasattr(payload, 'model_dump') else payload.dict()
    
    try:
        response = requests.post(url, json=payload_dict, timeout=1000)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Erreur lors de la suppression: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur serveur Data: {str(e)}")



@app.post("/train_full")
def training(payload: PaquetComplet, payload_model: dict):
    """Route d'entraÃ®nement complet avec le nouveau pipeline 3 phases"""
    pipeline = TrainingPipeline(payload, payload_model)
    return StreamingResponse(pipeline.execute_full_pipeline(), media_type="text/event-stream")

stop_training_flag = False

@app.post("/stop_training")
def stop_training():
    """ArrÃªte l'entraÃ®nement en cours"""
    global stop_training_flag
    stop_training_flag = True
    print("ðŸ›‘ ArrÃªt de l'entraÃ®nement demandÃ©")
    return {"status": "ok", "message": "ArrÃªt demandÃ©"}


@app.get("/")
def root():
    return {"message": "Serveur IA actif !"}


# ====================================
# ENDPOINT DE PRÃ‰DICTION FUTURE
# ====================================
from pydantic import BaseModel as PydanticBaseModel
from scipy import stats

class PredictRequest(PydanticBaseModel):
    """RequÃªte pour la prÃ©diction future"""
    horizon: int = 10  # Nombre de pas Ã  prÃ©dire dans le futur
    confidence_level: float = 0.95  # Niveau de confiance pour les intervalles


@app.post("/predict")
def predict_future(request: PredictRequest):
    """
    PrÃ©dit H pas dans le FUTUR en utilisant le modÃ¨le entraÃ®nÃ©.
    
    - Utilise les donnÃ©es chargÃ©es (payload_json) comme historique
    - PrÃ©dit au-delÃ  de la fin des donnÃ©es (pas de comparaison possible)
    - Retourne les prÃ©dictions avec intervalles de confiance
    """
    
    def prediction_generator():
        global trained_model_state, payload_json
        
        # VÃ©rifier qu'un modÃ¨le est disponible
        if not trained_model_state["is_trained"] or trained_model_state["model"] is None:
            yield sse({
                "type": "error",
                "message": "Aucun modÃ¨le entraÃ®nÃ© disponible. Veuillez d'abord entraÃ®ner un modÃ¨le via l'onglet Training."
            })
            return
        
        # VÃ©rifier que les donnÃ©es sont chargÃ©es
        if not payload_json.get("values") or len(payload_json["values"]) == 0:
            yield sse({
                "type": "error", 
                "message": "Aucune donnÃ©e chargÃ©e. Veuillez d'abord sÃ©lectionner un dataset."
            })
            return
        
        try:
            model = trained_model_state["model"]
            norm_params = trained_model_state["norm_params"]
            inverse_fn = trained_model_state["inverse_fn"]
            window_size = trained_model_state["window_size"]
            residual_std = trained_model_state["residual_std"] or 0.1
            model_type = trained_model_state["model_type"]
            device_str = trained_model_state["device"]
            
            # RÃ©cupÃ©rer le device
            device = torch.device(device_str if device_str != "cpu" else "cpu")
            
            # Utiliser les donnÃ©es du payload_json (chargÃ©es quand on sÃ©lectionne un dataset)
            series_values = payload_json["values"]
            
            horizon = request.horizon
            
            print(f"[PREDICT] DÃ©marrage prÃ©diction: horizon={horizon}, series_length={len(series_values)}, window_size={window_size}")
            
            yield sse({
                "type": "pred_start",
                "message": f"PrÃ©diction de {horizon} pas dans le futur",
                "n_steps": horizon,
                "series_length": len(series_values),
                "window_size": window_size
            })
            
            # PrÃ©parer les donnÃ©es
            series_array = np.array(series_values, dtype=np.float32)
            
            # Normaliser selon la mÃ©thode utilisÃ©e Ã  l'entraÃ®nement
            method = norm_params.get("method", "standardization")
            
            if method == "minmax":
                min_val = norm_params["min"]
                max_val = norm_params["max"]
                series_norm = (series_array - min_val) / (max_val - min_val + 1e-8)
            elif method in ["zscore", "standardization"]:
                mean_val = norm_params["mean"]
                std_val = norm_params["std"]
                series_norm = (series_array - mean_val) / (std_val + 1e-8)
            else:
                # Fallback
                mean_val = norm_params.get("mean", series_array.mean())
                std_val = norm_params.get("std", series_array.std())
                series_norm = (series_array - mean_val) / (std_val + 1e-8)
            
            # Contexte initial = derniÃ¨res valeurs normalisÃ©es
            context = series_norm[-window_size:].copy()
            
            predictions = []
            pred_low = []
            pred_high = []
            
            model.eval()
            
            # Z-score pour l'intervalle de confiance
            z_score = stats.norm.ppf((1 + request.confidence_level) / 2)
            
            with torch.no_grad():
                for step in range(horizon):
                    # PrÃ©parer l'entrÃ©e selon le type de modÃ¨le
                    x_input = torch.tensor(context, dtype=torch.float32).unsqueeze(0)
                    
                    if model_type == "lstm":
                        x_input = x_input.unsqueeze(-1)  # (1, window, 1)
                    elif model_type == "cnn":
                        x_input = x_input.unsqueeze(1)   # (1, 1, window)
                    
                    x_input = x_input.to(device)
                    
                    # PrÃ©diction
                    y_pred_norm = model(x_input)
                    y_pred_norm_val = y_pred_norm.cpu().numpy().flatten()[0]
                    
                    # DÃ©normaliser
                    if inverse_fn:
                        y_pred = inverse_fn(y_pred_norm_val)
                    else:
                        # DÃ©normalisation manuelle
                        if method == "minmax":
                            y_pred = y_pred_norm_val * (max_val - min_val) + min_val
                        else:
                            y_pred = y_pred_norm_val * std_val + mean_val
                    
                    # Intervalles de confiance (s'Ã©largissent avec le temps)
                    uncertainty = residual_std * z_score * np.sqrt(step + 1)
                    low = float(y_pred - uncertainty)
                    high = float(y_pred + uncertainty)
                    
                    predictions.append(float(y_pred))
                    pred_low.append(low)
                    pred_high.append(high)
                    
                    # Envoyer le point
                    yield sse({
                        "type": "pred_point",
                        "step": step + 1,
                        "yhat": float(y_pred),
                        "low": low,
                        "high": high,
                        "idx": len(series_values) + step
                    })
                    
                    # Mettre Ã  jour le contexte (autorÃ©gression)
                    context = np.roll(context, -1)
                    context[-1] = y_pred_norm_val
            
            print(f"[PREDICT] TerminÃ©: {len(predictions)} prÃ©dictions gÃ©nÃ©rÃ©es")
            
            # DonnÃ©es finales
            yield sse({
                "type": "pred_end",
                "message": f"PrÃ©diction terminÃ©e: {horizon} pas",
                "predictions": predictions,
                "pred_low": pred_low,
                "pred_high": pred_high,
                "idx_start": len(series_values),
                "series_complete": series_values,
                "horizon": horizon
            })
            
            yield sse({"type": "fin_prediction", "done": 1})
            
        except Exception as e:
            import traceback
            print(f"[PREDICT] ERREUR: {e}")
            traceback.print_exc()
            yield sse({
                "type": "error",
                "message": str(e),
                "traceback": traceback.format_exc()
            })
    
    return StreamingResponse(prediction_generator(), media_type="text/event-stream")


@app.get("/model/status")
def model_status():
    """Retourne le statut du modÃ¨le entraÃ®nÃ© et des donnÃ©es"""
    global trained_model_state, payload_json
    
    data_loaded = payload_json.get("values") and len(payload_json["values"]) > 0
    
    return {
        "model": {
            "is_trained": trained_model_state["is_trained"],
            "model_type": trained_model_state["model_type"],
            "window_size": trained_model_state["window_size"],
            "residual_std": trained_model_state["residual_std"],
            "device": trained_model_state["device"]
        },
        "data": {
            "is_loaded": data_loaded,
            "n_points": len(payload_json["values"]) if data_loaded else 0
        }
    }